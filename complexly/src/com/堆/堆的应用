 一、优先级队列:
      队列最大的特性就是先进先出。
    在优先级队列中，出队的顺序是按照优先级来，优先级最高的，最先出队。
  用堆来实现是最直接、最高效的优先级队列。因为堆和优先级队列非常相似。
  往优先级队列中插入一个元素，就相当于往堆中插入一个元素，取出优先级最高的元素，就相当于取出堆顶元素。
  应用场景:赫尔曼编码、图的最短路径、最小生成树算法。
  API实现:java中util包里的PriorityQueue，C++中的priority_queue等。

  使用:
  1.合并有序小文件:
   假设有100个小文件，每个文件的大小时100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。
   (1)可以使用数组:
   我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组删除。
   然后再从删除字符串的这个小文件中取下一个字符串，再放入数组中重新比较大小。直到所有的文件中的数据都放到大文件为止。
   缺点:每次从数组中取最小字符串，都需要循环遍历整个数组。
   (2).优先级队列（堆）:
   我们将从小文件取出来的字符串放入到小顶堆中，堆顶元素就是队列队首元素，也是最小的字符串。
   我们将这个字符串放入到大文件中，并将其从堆中删除，然后再从小文件中取下一个字符串放入到堆中。
   循环这个过程，就可以将100个小文件中的字符串依次放入到打文件中。
   而且删除堆顶元素和往堆中插入数据的时间复杂度都是O(logN).
  2.高性能定时器:
   假设有一个定时器，维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。
   (1).定时扫描:
    定时器每个1秒就扫描一遍任务，看是否有任务到达设定的执行时间，如果到达了，就拿出来执行。
   缺点:1.任务的约定执行时间离当前时间可能还有很久，这样扫描其实是徒劳的。2.每次都要扫描整个任务列表，列表很大的话，比较耗时。
   (2).优先级队列:
   我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列队首存储的是最先执行的任务。
   就不需要每隔1秒就扫描一遍任务列表了，只需要拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。
   这个T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。
   这样，定时器就可以设定在T秒之后，再来执行任务。到T秒前，定时器都不需要做任务事情。
   执行完队首任务，再计算新的队首任务的执行时间点与当前时间点的差值。

  二、利用堆求Top K:
   1.针对静态数据集合(数据集合事先确定):
    如果在一个包含n个数据的数组中，查找前K大数据呢?
      维护一个大小为K的小顶堆，顺序遍历数组，从数组中取数据与堆顶元素比较。
      如果比堆顶元素大，就把堆顶元素删除，将这个元素插入到堆中。如果比堆顶元素小，则继续遍历数组。
      这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。
      遍历数组需要O(n)的时间复杂度，一次堆化需要O(logK)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度是O(nlogK).

   2.针对动态数据集合(有数据动态加入到集合中):
    我们可以一直维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿去与堆顶元素对比。
    如果比堆顶元素大，就把堆顶元素删除，并将这个元素插入到堆中。如果比堆顶元素小，则不做处理。
    这样，无论任何时间需要查询当前的前K大数据，我们都可以立刻返回。


  三、有一个包含10亿个搜索关键词的日志文件，如何快速获取到Top 10 最热门的搜索关键词呢？
     可以利用大数据中的MapReduce来处理。但是场景限定为单机，可以使用的内存为1GB呢？
     因为用户搜索关键词很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。

     我们可以通过散列表、平衡二叉查找树(红黑树)来记录关键词及其出现的次数。
     散列表:顺序扫描10亿个关键词，当扫描到某个关键词时，去散列表中查询。如果存在，就将对应的次数+1;
     如果不存在，就插入到散列表中，并记录次数为1.等遍历完10亿个搜索关键词之后，散列表存储了不重复的搜素关键词以及出现的次数。
     我们再根据用堆求TopK的方法，建立一个大小为10的小顶堆，遍历散列表，一次取出每个搜索关键词以及对应出现的次数。
     与堆顶元素的搜索关键词对比，如果出现次数比堆顶搜索关键词的次数多，就删除堆顶关键词，将这个出现次数更多的关键词插入到堆中。
     当遍历完整个散列表中的关键词后，堆中的关键词就是出现次数最多的Top10搜索关键词了。

     有一个问题是:10亿的关键词，假设不重复的有1亿条，如果每个搜索关键词平均长度50个字节。
     那存储1亿个关键词起码需要5GB的内存空间，而散列表因为要避免散列冲突，不会选择太大的装载因子。
     所以消耗的内存空间就更多了。无法一次性将所有的关键词加入到内存中，怎么办呢？

     相同数据经过哈希算法得到的哈希值是一样的，我们将10亿条搜索关键词先通过哈希算法分片到10个文件中。
     :我们创建10个空文件，然后遍历10亿个关键词，通过某个哈希算法求哈希值，然后同10取模。分到不同的文件中。
     假设分片之后每个文件都只有1亿个关键词，去掉重复的，可能只有1000万个。
     我们再针对每个包含1亿条关键词的文件，利用散列表和堆，分别求出Top10，再把这10个Top10放在一起，
     取这100个关键词出现次数最多的10个关键词。


